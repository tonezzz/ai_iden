name: pgai

services:

  fastapi_yolo:
    container_name: fastapi_yolo
    image: fastapi_yolo:latest
    command: tail -F /var/log/alternatives.log
    environment:
      - YOLO_MODEL=ultralytics/yolov8n-pose
      - YOLO_DEVICE=cpu
    ports:
      - 8103:8000
    volumes:
      - ./run/yolo/data:/app/data

  pg:
    container_name: pg
    image: timescale/timescaledb-ha:pg17
    environment:
      POSTGRES_PASSWORD: postgres
    ports:
    - "5432:5432"
    volumes:
    - pg_data:/home/postgres/pgdata/data

  vectorizer-worker:
    profiles: [future]
    container_name: vectorizer-worker
    image: timescale/pgai-vectorizer-worker:v0.10.5
    environment:
      PGAI_VECTORIZER_WORKER_DB_URL: postgres://postgres:postgres@pg:5432/postgres
      OLLAMA_HOST: http://ollama:11434
    command: [ "--poll-interval", "5s" ]
    depends_on:
      - pg

  llama_fastapi:
    container_name: llama_fastapi
    image: ollama_fastapi:latest
    ports:
      - "8104:11434"
      - "8105:7869"
    volumes:
      - ./run/ollama/:/root/.ollama/
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all 
              capabilities: [gpu]

  ollama:
    container_name: ollama
    image: ollama/ollama:0.7.1
    volumes:
      - ./run/ollama/:/root/.ollama/
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all 
              capabilities: [gpu]

  llamaindex:
    container_name: llamaindex
    ports:
      - "3000:3000"
    #volumes:
    #  - ./run/llamaindex/cache:/usr/src/app/cache
    #  - ./chat-llamaindex/datasources:/usr/src/app/datasources
    #env_file:
    #  - ./chat-llamaindex/.env.dev
    build:
      context: ./llamaindex
      dockerfile_inline: |
        FROM python:3.11.9 AS my_llamaindex
        RUN apt update && apt install -y libgl1
        RUN pip install --upgrade pip

        # custom selection of integrations to work with core
        RUN pip install llama-index-core
        RUN pip install llama-index-llms-openai
        RUN pip install llama-index-llms-replicate
        RUN pip install llama-index-embeddings-huggingface

        WORKDIR /code

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all 
              capabilities: [gpu]

  ollama-webui:
    container_name: ollama-webui
    image: ghcr.io/open-webui/open-webui:main
    volumes:
      - ./run/ollama-webui/backend-data:/app/backend/data
    depends_on:
      - ollama
    ports:
      - 8102:8080
    environment: # https://docs.openwebui.com/getting-started/env-configuration#default_models
      #- OLLAMA_BASE_URLS=http://host.docker.internal:7869 #comma separated ollama hosts
      - OLLAMA_BASE_URLS=http://localhost:11434 #comma separated ollama hosts
      - ENV=dev
      - WEBUI_AUTH=False
      - WEBUI_NAME=valiantlynx AI
      - WEBUI_URL=http://localhost:8080
      - WEBUI_SECRET_KEY=t0p-s3cr3t
    #extra_hosts:
    #  - host.docker.internal:host-gateway
    #restart: unless-stopped
    #networks:
    #  - ollama-docker

  adminer:
    container_name: adminer
    image: adminer
    restart: always
    ports:
      - 8101:8080
    depends_on:
      - pg

  ngrok:
    container_name: ngrok
    environment:
      - NGROK_AUTHTOKEN=2dX3AiyaMZ9bc5JFBUPre9SRd0L_kKit9xXv5s7hf7E8k8Ei
    image: ngrok/ngrok:latest
    command: 'http fastapi:8100'
    ports:
      - 8109:80
    expose:
      - 8109
    depends_on:
      - fastapi

  fastapi:
    container_name: fastapi
    environment:
      - DEBUG=1
    volumes:
      - ./run/fastapi/config:/root/.config/Ultralytics
      - ./run/fastapi:/code
      - ./fastapi/app:/code/app
    ports:
      - 8100:8100
    build:
      context: .
      #dockerfile: ./fastapi/Dockerfile
      dockerfile_inline: |
        FROM python:3.11.9 AS my_python_311
        RUN apt update && apt install -y libgl1
        COPY ./fastapi/requirements.txt /code/requirements.txt
        RUN pip install --upgrade pip
        WORKDIR /code

        FROM my_python_311 AS my_python_311_ai 
        #RUN pip install -r /code/requirements.txt
        RUN pip install uvicorn[standard] psycopg2 pydantic
        RUN pip install torch torchvision numpy matplotlib pandas pillow psutil
        RUN pip install pyyaml scipy seaborn requests torch torchaudio tqdm
        RUN pip install opencv-python-headless

        FROM my_python_311_ai AS my_python_311_fastapi
        RUN pip install fastapi[standard]
        RUN pip install ultralytics-thop ultralytics
        RUN pip install supervision

        CMD ["fastapi", "run", "/code/app/main.py", "--port", "8100"]
       
        #COPY ./fastapi/app /code/app

volumes:
  pg_data:
